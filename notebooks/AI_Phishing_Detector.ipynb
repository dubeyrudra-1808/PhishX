{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "FFxJi9WOrO3K"
      },
      "outputs": [],
      "source": [
        "!mkdir -p data/raw data/processed models src notebooks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scikit-learn lightgbm tldextract beautifulsoup4 fastapi uvicorn joblib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUFgqUCtrZpZ",
        "outputId": "a1d6e00d-670d-4207-9599-8a32013d96ea"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.12/dist-packages (5.3.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.119.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.37.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from tldextract) (3.11)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from tldextract) (2.32.4)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.12/dist-packages (from tldextract) (3.0.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.12/dist-packages (from tldextract) (3.20.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.48.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi) (2.11.10)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.3.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.1.0->tldextract) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.1.0->tldextract) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.1.0->tldextract) (2025.10.5)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.49.0,>=0.40.0->fastapi) (4.11.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlUcNXUZry4C",
        "outputId": "730fd125-56d1-4cbc-f5eb-18608468e3c2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/AI_Phishing_Detector\n",
        "%cd /content/drive/MyDrive/AI_Phishing_Detector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16TFBHkIrzhw",
        "outputId": "0fdacf5b-a0e0-42f4-a8fb-4057fc39a84f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AI_Phishing_Detector\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data/raw data/processed models src notebooks"
      ],
      "metadata": {
        "id": "j0nm7GMVr8ou"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scikit-learn lightgbm tldextract beautifulsoup4 fastapi uvicorn joblib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E847fqUKsJBk",
        "outputId": "9803fbdd-6404-465d-f8c0-a2acd9d66fe8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.12/dist-packages (5.3.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.119.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.37.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from tldextract) (3.11)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from tldextract) (2.32.4)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.12/dist-packages (from tldextract) (3.0.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.12/dist-packages (from tldextract) (3.20.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.48.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi) (2.11.10)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.3.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.1.0->tldextract) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.1.0->tldextract) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.1.0->tldextract) (2025.10.5)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.49.0,>=0.40.0->fastapi) (4.11.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, numpy as np, sklearn, lightgbm, tldextract, bs4\n",
        "print(\"✅ Environment ready. All dependencies loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVIjO_M8sO5F",
        "outputId": "9795eaec-b49c-4d58-a7b7-8ce282821844"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Environment ready. All dependencies loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "output_dir = \"/content/drive/MyDrive/AI_Phishing_Detector/data/raw/\"\n",
        "output_file = os.path.join(output_dir, \"phishing_urls.csv\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Download CSV\n",
        "url = \"https://data.phishtank.com/data/online-valid.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df = df[['url']]  # Keep only URL column\n",
        "\n",
        "# Save to Drive\n",
        "df.to_csv(output_file, index=False)\n",
        "print(f\"Saved {len(df)} phishing URLs to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ma-ghJVrsSNX",
        "outputId": "3efeb370-7302-49ae-f036-5117b728868a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 49371 phishing URLs to /content/drive/MyDrive/AI_Phishing_Detector/data/raw/phishing_urls.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "output_dir = \"/content/drive/MyDrive/AI_Phishing_Detector/data/raw/\"\n",
        "output_file = os.path.join(output_dir, \"legit_urls.csv\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Tranco top 1M list\n",
        "tranco_url = \"https://tranco-list.eu/top-1m.csv.zip\"\n",
        "r = requests.get(tranco_url)\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "df = pd.read_csv(z.open('top-1m.csv'), header=None)\n",
        "df = df.head(50000)  # take top 50k domains\n",
        "\n",
        "# Convert domains to full URLs\n",
        "df['url'] = \"https://\" + df[1]\n",
        "df = df[['url']]\n",
        "\n",
        "# Save\n",
        "df.to_csv(output_file, index=False)\n",
        "print(f\"Saved {len(df)} legitimate URLs to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTaPEVTjt9DL",
        "outputId": "e9b40779-a14b-4d7b-8ef8-974d9d2c12db"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 50000 legitimate URLs to /content/drive/MyDrive/AI_Phishing_Detector/data/raw/legit_urls.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "output_dir = \"/content/drive/MyDrive/AI_Phishing_Detector/data/processed/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "output_file = os.path.join(output_dir, \"live_dataset.csv\")\n",
        "\n",
        "# Load raw data\n",
        "phish = pd.read_csv(\"/content/drive/MyDrive/AI_Phishing_Detector/data/raw/phishing_urls.csv\")\n",
        "legit = pd.read_csv(\"/content/drive/MyDrive/AI_Phishing_Detector/data/raw/legit_urls.csv\")\n",
        "\n",
        "# Assign labels\n",
        "phish['label'] = 1\n",
        "legit['label'] = 0\n",
        "\n",
        "# Merge datasets\n",
        "df = pd.concat([phish, legit], ignore_index=True)\n",
        "\n",
        "# Shuffle\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Remove duplicates (just in case)\n",
        "df = df.drop_duplicates(subset=['url'])\n",
        "\n",
        "# Save final dataset\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "# Quick verification\n",
        "print(f\"Final dataset shape: {df.shape}\")\n",
        "print(\"Class distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "BARmYvWdvUYb",
        "outputId": "d755e42b-0007-4b4b-a5f0-a57f0d8609f0"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final dataset shape: (99361, 2)\n",
            "Class distribution:\n",
            "label\n",
            "0    49999\n",
            "1    49362\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       url  label\n",
              "0       https://gbitt.info      0\n",
              "1        https://xmrig.com      0\n",
              "2  https://l.ead.me/bfUXDb      1\n",
              "3       https://yximgs.com      0\n",
              "4   https://transitcard.ru      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c5027c4c-0cda-4adf-8154-40cf8853ff13\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://gbitt.info</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://xmrig.com</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://l.ead.me/bfUXDb</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://yximgs.com</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://transitcard.ru</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c5027c4c-0cda-4adf-8154-40cf8853ff13')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c5027c4c-0cda-4adf-8154-40cf8853ff13 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c5027c4c-0cda-4adf-8154-40cf8853ff13');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-dfc18ccf-1d2d-4bfd-a7dc-6a1b4e8591a9\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dfc18ccf-1d2d-4bfd-a7dc-6a1b4e8591a9')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-dfc18ccf-1d2d-4bfd-a7dc-6a1b4e8591a9 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 99361,\n  \"fields\": [\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 99361,\n        \"samples\": [\n          \"https://kraken-apps.firebaseapp.com/\",\n          \"https://nexdrama.com\",\n          \"https://google.mn\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# src/feature_extraction.py\n",
        "# Comprehensive, production-minded URL feature extractor for phishing detection.\n",
        "# - URL-only lexical & structural features (no page fetch)\n",
        "# - Optional light network checks (DNS resolve) controlled by perform_network_checks flag\n",
        "# - Character-sequence encoder for Char-CNN (fixed length, padding/truncation)\n",
        "# - Saves `features_df` and `char_seqs.npy` if asked\n",
        "#\n",
        "# Usage example (Colab):\n",
        "#   from src.feature_extraction import build_features_for_df\n",
        "#   df = pd.read_csv(\"/content/drive/MyDrive/AI_Phishing_Detector/data/processed/live_dataset.csv\")\n",
        "#   feats_df, char_seqs = build_features_for_df(df['url'], perform_network_checks=False, save_prefix=\"/content/drive/.../features\")\n",
        "#\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import json\n",
        "import hashlib\n",
        "import socket\n",
        "from urllib.parse import urlparse, unquote\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tldextract\n",
        "\n",
        "# ---------------------------\n",
        "# Configuration / constants\n",
        "# ---------------------------\n",
        "CHAR_VOCAB = (\n",
        "    list(\"abcdefghijklmnopqrstuvwxyz\") +\n",
        "    list(\"0123456789\") +\n",
        "    list(\":/?&=.%-_+#@~\")  # a compact set of common URL chars\n",
        ")\n",
        "# Add uppercase mapping to lowercase in tokenizer\n",
        "CHAR_TO_INDEX = {c: i+1 for i, c in enumerate(CHAR_VOCAB)}  # reserve 0 for padding\n",
        "UNK_CHAR_INDEX = len(CHAR_TO_INDEX) + 1\n",
        "DEFAULT_MAX_LEN = 200\n",
        "\n",
        "# Suspicious TLDs (expandable). Use as a binary indicator.\n",
        "SUSPICIOUS_TLDS = {\n",
        "    'xyz', 'top', 'club', 'online', 'site', 'website', 'pw', 'tk', 'ml', 'ga', 'cf', 'gq'\n",
        "}\n",
        "\n",
        "# Regexes\n",
        "RE_IPv4 = re.compile(r'^(?:\\d{1,3}\\.){3}\\d{1,3}$')\n",
        "RE_IP_IN_HOST = re.compile(r'(^|\\[)(\\d{1,3}\\.){3}\\d{1,3}(\\]|$)')\n",
        "RE_PORT = re.compile(r':\\d+$')\n",
        "RE_PERCENT_ENCODE = re.compile(r'%[0-9a-fA-F]{2}')\n",
        "RE_NON_ALNUM = re.compile(r'[^A-Za-z0-9]')\n",
        "\n",
        "# ---------------------------\n",
        "# Utility functions\n",
        "# ---------------------------\n",
        "def normalize_url(url: str) -> str:\n",
        "    \"\"\"Lowercase scheme & host. Strip surrounding whitespace. Keep path/query intact.\"\"\"\n",
        "    if not isinstance(url, str):\n",
        "        return \"\"\n",
        "    url = url.strip()\n",
        "    if not url:\n",
        "        return \"\"\n",
        "    # ensure scheme present (default to https)\n",
        "    if not re.match(r'^[a-zA-Z]+://', url):\n",
        "        url = 'http://' + url  # use http to allow parsing; we keep original scheme presence as feature\n",
        "    parsed = urlparse(url)\n",
        "    # Rebuild with normalized netloc\n",
        "    netloc = parsed.netloc.lower()\n",
        "    rebuilt = parsed._replace(netloc=netloc).geturl()\n",
        "    return rebuilt\n",
        "\n",
        "def is_ip_host(host: str) -> int:\n",
        "    if not host:\n",
        "        return 0\n",
        "    # strip possible port\n",
        "    host_no_port = host.split(':')[0]\n",
        "    if RE_IPv4.match(host_no_port):\n",
        "        return 1\n",
        "    # also check for encoded/ip formats inside host\n",
        "    if RE_IP_IN_HOST.search(host):\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "def shannon_entropy(s: str) -> float:\n",
        "    if not s:\n",
        "        return 0.0\n",
        "    counts = Counter(s)\n",
        "    probs = [v/len(s) for v in counts.values()]\n",
        "    return -sum(p * math.log2(p) for p in probs) if probs else 0.0\n",
        "\n",
        "def count_special_chars(s: str) -> int:\n",
        "    return len(RE_NON_ALNUM.findall(s))\n",
        "\n",
        "def has_port(netloc: str) -> int:\n",
        "    # netloc may contain ':port'\n",
        "    if ':' in netloc and netloc.split(':')[-1].isdigit():\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "def extract_tld_info(domain: str) -> Tuple[str, str]:\n",
        "    \"\"\"Return (registered_domain, tld). registered_domain is e.g. example.co.uk -> example.co.uk\"\"\"\n",
        "    if not domain:\n",
        "        return \"\", \"\"\n",
        "    te = tldextract.extract(domain)\n",
        "    registered = \".\".join(part for part in [te.domain, te.suffix] if part)\n",
        "    return registered, te.suffix.lower() if te.suffix else \"\"\n",
        "\n",
        "def percent_encoded_fraction(s: str) -> float:\n",
        "    if not s:\n",
        "        return 0.0\n",
        "    matches = RE_PERCENT_ENCODE.findall(s)\n",
        "    return len(matches) / max(1, len(s))\n",
        "\n",
        "def count_digits(s: str) -> int:\n",
        "    return sum(ch.isdigit() for ch in s)\n",
        "\n",
        "def count_letters(s: str) -> int:\n",
        "    return sum(ch.isalpha() for ch in s)\n",
        "\n",
        "def vowel_fraction(s: str) -> float:\n",
        "    if not s:\n",
        "        return 0.0\n",
        "    return sum(ch in 'aeiou' for ch in s.lower()) / max(1, len(s))\n",
        "\n",
        "# ---------------------------\n",
        "# Optional network checks (use sparingly)\n",
        "# ---------------------------\n",
        "def try_resolve_domain(domain: str, timeout: float = 2.0) -> Dict[str, Optional[int]]:\n",
        "    \"\"\"Try resolving domain to IP(s). Returns dict with ip_count and resolved_one (0/1). Safe: exceptions handled.\"\"\"\n",
        "    result = {\"ip_count\": None, \"resolves\": None}\n",
        "    if not domain:\n",
        "        return result\n",
        "    try:\n",
        "        # set default timeout for sockets for safety\n",
        "        orig_timeout = socket.getdefaulttimeout()\n",
        "        socket.setdefaulttimeout(timeout)\n",
        "        infos = socket.getaddrinfo(domain, None)\n",
        "        ips = {info[4][0] for info in infos if info and info[4]}\n",
        "        result['ip_count'] = len(ips)\n",
        "        result['resolves'] = 1 if ips else 0\n",
        "    except Exception:\n",
        "        result['ip_count'] = 0\n",
        "        result['resolves'] = 0\n",
        "    finally:\n",
        "        socket.setdefaulttimeout(orig_timeout)\n",
        "    return result\n",
        "\n",
        "# ---------------------------\n",
        "# Core feature extraction per URL\n",
        "# ---------------------------\n",
        "def extract_lexical_features(url: str, perform_network_checks: bool = False) -> Dict[str, object]:\n",
        "    \"\"\"\n",
        "    Extract a dict of features for a single URL.\n",
        "    perform_network_checks: if True, includes lightweight DNS resolution features (may slow down).\n",
        "    \"\"\"\n",
        "    record = {}\n",
        "    if not url or not isinstance(url, str):\n",
        "        # return zeroed features\n",
        "        keys = [\n",
        "            'url_length','host_length','path_length','num_dots','num_path_segments','num_query_params',\n",
        "            'count_digits','count_letters','digit_letter_ratio','count_special_chars','num_hyphens','has_at',\n",
        "            'has_ip_in_host','tld','suspicious_tld','entropy_host','entropy_path','percent_encoded_frac',\n",
        "            'vowel_frac','has_https','has_port','has_fragment'\n",
        "        ]\n",
        "        return {k: 0 for k in keys}\n",
        "\n",
        "    norm = normalize_url(url)\n",
        "    parsed = urlparse(norm)\n",
        "    scheme = parsed.scheme.lower()\n",
        "    netloc = parsed.netloc\n",
        "    path = parsed.path or \"\"\n",
        "    query = parsed.query or \"\"\n",
        "    fragment = parsed.fragment or \"\"\n",
        "    hostname = netloc.split('@')[-1].split(':')[0]  # strip possible credentials and port\n",
        "\n",
        "    # basic lengths\n",
        "    record['url_length'] = len(norm)\n",
        "    record['host_length'] = len(hostname)\n",
        "    record['path_length'] = len(path)\n",
        "    record['num_dots'] = hostname.count('.')\n",
        "    # path segments\n",
        "    record['num_path_segments'] = len([p for p in path.split('/') if p])\n",
        "    # query params count (rough)\n",
        "    record['num_query_params'] = len([p for p in query.split('&') if '=' in p]) if query else 0\n",
        "\n",
        "    # counts\n",
        "    record['count_digits'] = count_digits(norm)\n",
        "    record['count_letters'] = count_letters(norm)\n",
        "    record['digit_letter_ratio'] = (record['count_digits'] / (record['count_letters'] + 1))\n",
        "    record['count_special_chars'] = count_special_chars(norm)\n",
        "    record['num_hyphens'] = norm.count('-')\n",
        "    record['has_at'] = 1 if '@' in norm else 0\n",
        "\n",
        "    # host checks\n",
        "    record['has_ip_in_host'] = is_ip_host(hostname)\n",
        "    registered_domain, tld = extract_tld_info(hostname)\n",
        "    record['registered_domain'] = registered_domain\n",
        "    record['tld'] = tld\n",
        "    record['suspicious_tld'] = 1 if (tld in SUSPICIOUS_TLDS) else 0\n",
        "\n",
        "    # entropy & encoding\n",
        "    record['entropy_host'] = shannon_entropy(hostname)\n",
        "    record['entropy_path'] = shannon_entropy(path)\n",
        "    record['percent_encoded_frac'] = percent_encoded_fraction(norm)\n",
        "\n",
        "    # misc heuristics\n",
        "    record['vowel_frac'] = vowel_fraction(hostname)\n",
        "    record['has_https'] = 1 if scheme == 'https' else 0\n",
        "    record['has_port'] = has_port(netloc)\n",
        "    record['has_fragment'] = 1 if fragment else 0\n",
        "\n",
        "    # heuristic suspicious tokens in URL/domain\n",
        "    suspicious_tokens = ['login', 'signin', 'secure', 'webscr', 'bank', 'verify', 'update', 'account', 'confirm']\n",
        "    url_lower = norm.lower()\n",
        "    for tok in suspicious_tokens:\n",
        "        record[f'token_{tok}'] = 1 if tok in url_lower else 0\n",
        "\n",
        "    # length buckets (useful as categorical-ish numeric)\n",
        "    record['url_len_bucket_<50'] = 1 if record['url_length'] < 50 else 0\n",
        "    record['url_len_bucket_50_100'] = 1 if 50 <= record['url_length'] < 100 else 0\n",
        "    record['url_len_bucket_100_200'] = 1 if 100 <= record['url_length'] < 200 else 0\n",
        "    record['url_len_bucket_>=200'] = 1 if record['url_length'] >= 200 else 0\n",
        "\n",
        "    # network checks (optional & cached by user)\n",
        "    if perform_network_checks:\n",
        "        netinfo = try_resolve_domain(hostname)\n",
        "        record['dns_ip_count'] = netinfo.get('ip_count', 0)\n",
        "        record['dns_resolves'] = netinfo.get('resolves', 0)\n",
        "    else:\n",
        "        record['dns_ip_count'] = None\n",
        "        record['dns_resolves'] = None\n",
        "\n",
        "    return record\n",
        "\n",
        "# ---------------------------\n",
        "# Batch processing helpers\n",
        "# ---------------------------\n",
        "def build_features_for_df(url_series: pd.Series,\n",
        "                          perform_network_checks: bool = False,\n",
        "                          char_maxlen: int = DEFAULT_MAX_LEN,\n",
        "                          save_prefix: Optional[str] = None\n",
        "                         ) -> Tuple[pd.DataFrame, np.ndarray, Dict]:\n",
        "    \"\"\"\n",
        "    Given a pandas Series of URLs, returns:\n",
        "      - features_df: DataFrame with extracted features (one row per URL)\n",
        "      - char_seqs: numpy array shape (n_urls, char_maxlen) with integer indices for Char-CNN\n",
        "      - meta: dict containing char_vocab mapping and any notes\n",
        "\n",
        "    If save_prefix provided (string path prefix), will save:\n",
        "      - {save_prefix}_features.csv\n",
        "      - {save_prefix}_char_seqs.npy\n",
        "      - {save_prefix}_meta.json\n",
        "    \"\"\"\n",
        "    urls = url_series.fillna(\"\").astype(str).tolist()\n",
        "    n = len(urls)\n",
        "\n",
        "    # Extract lexical features\n",
        "    feats = []\n",
        "    for i, u in enumerate(urls):\n",
        "        feats.append(extract_lexical_features(u, perform_network_checks=perform_network_checks))\n",
        "        if (i + 1) % 5000 == 0:\n",
        "            print(f\"Processed {i+1}/{n} URLs...\")\n",
        "\n",
        "    features_df = pd.DataFrame(feats)\n",
        "\n",
        "    # Drop columns that are non-numeric or not needed for ML; keep registered_domain and tld optionally\n",
        "    # We'll keep registered_domain for reference but ML models should not get it raw (hash or drop later)\n",
        "    # Convert None to np.nan as LightGBM handles nan\n",
        "    features_df = features_df.replace({None: np.nan})\n",
        "\n",
        "    # Build char sequences\n",
        "    char_seqs = np.zeros((n, char_maxlen), dtype=np.int32)\n",
        "    for i, u in enumerate(urls):\n",
        "        seq = encode_url_to_char_indices(u, maxlen=char_maxlen)\n",
        "        char_seqs[i, :] = seq\n",
        "\n",
        "    meta = {\n",
        "        \"char_to_index\": CHAR_TO_INDEX,\n",
        "        \"unk_char_index\": UNK_CHAR_INDEX,\n",
        "        \"maxlen\": char_maxlen\n",
        "    }\n",
        "\n",
        "    # Optional saving\n",
        "    if save_prefix:\n",
        "        os.makedirs(os.path.dirname(save_prefix), exist_ok=True)\n",
        "        features_csv = f\"{save_prefix}_features.csv\"\n",
        "        char_npy = f\"{save_prefix}_char_seqs.npy\"\n",
        "        meta_json = f\"{save_prefix}_meta.json\"\n",
        "        features_df.to_csv(features_csv, index=False)\n",
        "        np.save(char_npy, char_seqs)\n",
        "        with open(meta_json, \"w\") as f:\n",
        "            json.dump(meta, f)\n",
        "        print(f\"Saved features to {features_csv}, char seqs to {char_npy}, meta to {meta_json}\")\n",
        "\n",
        "    return features_df, char_seqs, meta\n",
        "\n",
        "# ---------------------------\n",
        "# Character encoding for Char-CNN\n",
        "# ---------------------------\n",
        "def encode_url_to_char_indices(url: str, maxlen: int = DEFAULT_MAX_LEN) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Turn URL into fixed-length vector of integer token ids.\n",
        "    - Lowercases before mapping\n",
        "    - Unknown chars map to UNK_CHAR_INDEX\n",
        "    - Right-pad with zeros (index 0 reserved for padding)\n",
        "    \"\"\"\n",
        "    if not isinstance(url, str):\n",
        "        url = \"\"\n",
        "    # normalize a bit but keep raw path/params signs\n",
        "    url = normalize_url(url)\n",
        "    url = url.lower()\n",
        "    seq = np.zeros(maxlen, dtype=np.int32)\n",
        "    # truncate from left → keep rightmost part (commonly path + file)\n",
        "    u = url[-maxlen:]\n",
        "    for i, ch in enumerate(u):\n",
        "        idx = CHAR_TO_INDEX.get(ch)\n",
        "        if idx is None:\n",
        "            idx = UNK_CHAR_INDEX\n",
        "        seq[i] = idx\n",
        "    return seq\n",
        "\n",
        "# ---------------------------\n",
        "# Small helper for hashing domains (for logging without storing raw URL)\n",
        "# ---------------------------\n",
        "def salted_hash(value: str, salt: str = \"static_salt_for_demo\") -> str:\n",
        "    if not isinstance(value, str):\n",
        "        value = \"\"\n",
        "    return hashlib.sha256((salt + value).encode()).hexdigest()\n",
        "\n",
        "# ---------------------------\n",
        "# Quick test utility\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Simple local smoke test (not heavy)\n",
        "    sample = [\n",
        "        \"http://192.168.0.1/login?user=abc\",\n",
        "        \"https://www.paypal.com/signin\",\n",
        "        \"http://secure-google-accounts.xyz/verify\",\n",
        "        \"https://accounts.google.com/ServiceLogin\",\n",
        "        \"phishing-domain.tk/login.php?acc=1\"\n",
        "    ]\n",
        "    df = pd.DataFrame({'url': sample})\n",
        "    feats, chars, meta = build_features_for_df(df['url'], perform_network_checks=False, save_prefix=None)\n",
        "    print(\"Features head:\")\n",
        "    print(feats.head())\n",
        "    print(\"Char seqs shape:\", chars.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPmxUp3bvfBY",
        "outputId": "11922f8b-a249-424f-c179-6daa60f1cd24"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features head:\n",
            "   url_length  host_length  path_length  num_dots  num_path_segments  \\\n",
            "0          33           11            6         3                  1   \n",
            "1          29           14            7         2                  1   \n",
            "2          40           26            7         1                  1   \n",
            "3          40           19           13         2                  1   \n",
            "4          41           18           10         1                  1   \n",
            "\n",
            "   num_query_params  count_digits  count_letters  digit_letter_ratio  \\\n",
            "0                 1             8             16            0.470588   \n",
            "1                 0             0             23            0.000000   \n",
            "2                 0             0             33            0.000000   \n",
            "3                 0             0             34            0.000000   \n",
            "4                 1             1             31            0.031250   \n",
            "\n",
            "   count_special_chars  ...  token_verify  token_update  token_account  \\\n",
            "0                    9  ...             0             0              0   \n",
            "1                    6  ...             0             0              0   \n",
            "2                    7  ...             1             0              1   \n",
            "3                    6  ...             0             0              1   \n",
            "4                    9  ...             0             0              0   \n",
            "\n",
            "  token_confirm url_len_bucket_<50  url_len_bucket_50_100  \\\n",
            "0             0                  1                      0   \n",
            "1             0                  1                      0   \n",
            "2             0                  1                      0   \n",
            "3             0                  1                      0   \n",
            "4             0                  1                      0   \n",
            "\n",
            "   url_len_bucket_100_200  url_len_bucket_>=200  dns_ip_count  dns_resolves  \n",
            "0                       0                     0           NaN           NaN  \n",
            "1                       0                     0           NaN           NaN  \n",
            "2                       0                     0           NaN           NaN  \n",
            "3                       0                     0           NaN           NaN  \n",
            "4                       0                     0           NaN           NaN  \n",
            "\n",
            "[5 rows x 38 columns]\n",
            "Char seqs shape: (5, 200)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2065782859.py:268: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  features_df = features_df.replace({None: np.nan})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir -p /content/drive/MyDrive/AI_Phishing_Detector/src\n",
        "cat > /content/drive/MyDrive/AI_Phishing_Detector/src/feature_extraction.py <<'PY'\n",
        "# Paste the entire feature_extraction.py content you provided here.\n",
        "# (If it is already present, this will overwrite it with the same content.)\n",
        "# --- START OF FILE ---\n",
        "# [PASTE YOUR FEATURE EXTRACTION CODE BLOCK HERE]\n",
        "# --- END OF FILE ---\n",
        "PY\n",
        "echo \"Saved /content/drive/MyDrive/AI_Phishing_Detector/src/feature_extraction.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZM27ymjOw_Bs",
        "outputId": "e4a4f32e-50c4-4bf0-c4ad-8f84dfbfbc48"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved /content/drive/MyDrive/AI_Phishing_Detector/src/feature_extraction.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "base = Path(\"/content/drive/MyDrive/AI_Phishing_Detector\")\n",
        "raw_phish = base/\"data\"/\"raw\"/\"phishing_urls.csv\"\n",
        "raw_legit = base/\"data\"/\"raw\"/\"legit_urls.csv\"\n",
        "\n",
        "phish = pd.read_csv(raw_phish)\n",
        "legit = pd.read_csv(raw_legit)\n",
        "\n",
        "phish['label'] = 1\n",
        "legit['label'] = 0\n",
        "\n",
        "df = pd.concat([phish, legit], ignore_index=True)\n",
        "df = df.drop_duplicates(subset=['url']).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"Combined shape:\", df.shape)\n",
        "display(df.head(5))\n",
        "# Save the combined file so it's reproducible\n",
        "proc_dir = base/\"data\"/\"processed\"\n",
        "proc_dir.mkdir(parents=True, exist_ok=True)\n",
        "df.to_csv(proc_dir/\"live_dataset.csv\", index=False)\n",
        "print(\"Saved:\", proc_dir/\"live_dataset.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "LmJE-tFpyTwZ",
        "outputId": "ce97b242-3d40-4745-c8e6-7ae41cc56441"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined shape: (99361, 2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                 url  label\n",
              "0  http://strt.gaestehauskrasse.com/challenge.php...      1\n",
              "1                     http://notificacionbcr.0hi.me/      1\n",
              "2                         https://productmadness.com      0\n",
              "3                        https://kupujemprodajem.com      0\n",
              "4                                  https://csudh.edu      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f3558faf-8ab0-4293-805b-e7f76e5db534\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http://strt.gaestehauskrasse.com/challenge.php...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>http://notificacionbcr.0hi.me/</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://productmadness.com</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://kupujemprodajem.com</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://csudh.edu</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f3558faf-8ab0-4293-805b-e7f76e5db534')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f3558faf-8ab0-4293-805b-e7f76e5db534 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f3558faf-8ab0-4293-805b-e7f76e5db534');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e703fd14-13bf-4849-8761-2e35a7074e71\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e703fd14-13bf-4849-8761-2e35a7074e71')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e703fd14-13bf-4849-8761-2e35a7074e71 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"Saved:\\\", proc_dir/\\\"live_dataset\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"http://notificacionbcr.0hi.me/\",\n          \"https://csudh.edu\",\n          \"https://productmadness.com\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/AI_Phishing_Detector/data/processed/live_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/AI_Phishing_Detector/src/feature_extraction.py\"\n",
        "print(\"Exists:\", __import__(\"os\").path.exists(path))\n",
        "print(\"\\n--- file head (1-200) ---\")\n",
        "!sed -n '1,200p' \"/content/drive/MyDrive/AI_Phishing_Detector/src/feature_extraction.py\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sN1IFaNBzEAy",
        "outputId": "ef4979a5-45b1-4196-edaa-7c238a9309f9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exists: True\n",
            "\n",
            "--- file head (1-200) ---\n",
            "# Paste the entire feature_extraction.py content you provided here.\n",
            "# (If it is already present, this will overwrite it with the same content.)\n",
            "# --- START OF FILE ---\n",
            "# [PASTE YOUR FEATURE EXTRACTION CODE BLOCK HERE]\n",
            "# --- END OF FILE ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > /content/drive/MyDrive/AI_Phishing_Detector/src/feature_extraction.py <<'PY'\n",
        "# src/feature_extraction.py\n",
        "# Comprehensive, production-minded URL feature extractor for phishing detection.\n",
        "# - URL-only lexical & structural features (no page fetch)\n",
        "# - Optional light network checks (DNS resolve) controlled by perform_network_checks flag\n",
        "# - Character-sequence encoder for Char-CNN (fixed length, padding/truncation)\n",
        "# - Saves `features_df` and `char_seqs.npy` if asked\n",
        "#\n",
        "# Usage example (Colab):\n",
        "#   from src.feature_extraction import build_features_for_df\n",
        "#   df = pd.read_csv(\"/content/drive/MyDrive/AI_Phishing_Detector/data/processed/live_dataset.csv\")\n",
        "#   feats_df, char_seqs = build_features_for_df(df['url'], perform_network_checks=False, save_prefix=\"/content/drive/.../features\")\n",
        "#\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import json\n",
        "import hashlib\n",
        "import socket\n",
        "from urllib.parse import urlparse, unquote\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tldextract\n",
        "\n",
        "# ---------------------------\n",
        "# Configuration / constants\n",
        "# ---------------------------\n",
        "CHAR_VOCAB = (\n",
        "    list(\"abcdefghijklmnopqrstuvwxyz\") +\n",
        "    list(\"0123456789\") +\n",
        "    list(\":/?&=.%-_+#@~\")  # a compact set of common URL chars\n",
        ")\n",
        "# Add uppercase mapping to lowercase in tokenizer\n",
        "CHAR_TO_INDEX = {c: i+1 for i, c in enumerate(CHAR_VOCAB)}  # reserve 0 for padding\n",
        "UNK_CHAR_INDEX = len(CHAR_TO_INDEX) + 1\n",
        "DEFAULT_MAX_LEN = 200\n",
        "\n",
        "# Suspicious TLDs (expandable). Use as a binary indicator.\n",
        "SUSPICIOUS_TLDS = {\n",
        "    'xyz', 'top', 'club', 'online', 'site', 'website', 'pw', 'tk', 'ml', 'ga', 'cf', 'gq'\n",
        "}\n",
        "\n",
        "# Regexes\n",
        "RE_IPv4 = re.compile(r'^(?:\\d{1,3}\\.){3}\\d{1,3}$')\n",
        "RE_IP_IN_HOST = re.compile(r'(^|\\[)(\\d{1,3}\\.){3}\\d{1,3}(\\]|$)')\n",
        "RE_PORT = re.compile(r':\\d+$')\n",
        "RE_PERCENT_ENCODE = re.compile(r'%[0-9a-fA-F]{2}')\n",
        "RE_NON_ALNUM = re.compile(r'[^A-Za-z0-9]')\n",
        "\n",
        "# ---------------------------\n",
        "# Utility functions\n",
        "# ---------------------------\n",
        "def normalize_url(url: str) -> str:\n",
        "    \"\"\"Lowercase scheme & host. Strip surrounding whitespace. Keep path/query intact.\"\"\"\n",
        "    if not isinstance(url, str):\n",
        "        return \"\"\n",
        "    url = url.strip()\n",
        "    if not url:\n",
        "        return \"\"\n",
        "    # ensure scheme present (default to https)\n",
        "    if not re.match(r'^[a-zA-Z]+://', url):\n",
        "        url = 'http://' + url  # use http to allow parsing; we keep original scheme presence as feature\n",
        "    parsed = urlparse(url)\n",
        "    # Rebuild with normalized netloc\n",
        "    netloc = parsed.netloc.lower()\n",
        "    rebuilt = parsed._replace(netloc=netloc).geturl()\n",
        "    return rebuilt\n",
        "\n",
        "def is_ip_host(host: str) -> int:\n",
        "    if not host:\n",
        "        return 0\n",
        "    # strip possible port\n",
        "    host_no_port = host.split(':')[0]\n",
        "    if RE_IPv4.match(host_no_port):\n",
        "        return 1\n",
        "    # also check for encoded/ip formats inside host\n",
        "    if RE_IP_IN_HOST.search(host):\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "def shannon_entropy(s: str) -> float:\n",
        "    if not s:\n",
        "        return 0.0\n",
        "    counts = Counter(s)\n",
        "    probs = [v/len(s) for v in counts.values()]\n",
        "    return -sum(p * math.log2(p) for p in probs) if probs else 0.0\n",
        "\n",
        "def count_special_chars(s: str) -> int:\n",
        "    return len(RE_NON_ALNUM.findall(s))\n",
        "\n",
        "def has_port(netloc: str) -> int:\n",
        "    # netloc may contain ':port'\n",
        "    if ':' in netloc and netloc.split(':')[-1].isdigit():\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "def extract_tld_info(domain: str) -> Tuple[str, str]:\n",
        "    \"\"\"Return (registered_domain, tld). registered_domain is e.g. example.co.uk -> example.co.uk\"\"\"\n",
        "    if not domain:\n",
        "        return \"\", \"\"\n",
        "    te = tldextract.extract(domain)\n",
        "    registered = \".\".join(part for part in [te.domain, te.suffix] if part)\n",
        "    return registered, te.suffix.lower() if te.suffix else \"\"\n",
        "\n",
        "def percent_encoded_fraction(s: str) -> float:\n",
        "    if not s:\n",
        "        return 0.0\n",
        "    matches = RE_PERCENT_ENCODE.findall(s)\n",
        "    return len(matches) / max(1, len(s))\n",
        "\n",
        "def count_digits(s: str) -> int:\n",
        "    return sum(ch.isdigit() for ch in s)\n",
        "\n",
        "def count_letters(s: str) -> int:\n",
        "    return sum(ch.isalpha() for ch in s)\n",
        "\n",
        "def vowel_fraction(s: str) -> float:\n",
        "    if not s:\n",
        "        return 0.0\n",
        "    return sum(ch in 'aeiou' for ch in s.lower()) / max(1, len(s))\n",
        "\n",
        "# ---------------------------\n",
        "# Optional network checks (use sparingly)\n",
        "# ---------------------------\n",
        "def try_resolve_domain(domain: str, timeout: float = 2.0) -> Dict[str, Optional[int]]:\n",
        "    \"\"\"Try resolving domain to IP(s). Returns dict with ip_count and resolved_one (0/1). Safe: exceptions handled.\"\"\"\n",
        "    result = {\"ip_count\": None, \"resolves\": None}\n",
        "    if not domain:\n",
        "        return result\n",
        "    try:\n",
        "        # set default timeout for sockets for safety\n",
        "        orig_timeout = socket.getdefaulttimeout()\n",
        "        socket.setdefaulttimeout(timeout)\n",
        "        infos = socket.getaddrinfo(domain, None)\n",
        "        ips = {info[4][0] for info in infos if info and info[4]}\n",
        "        result['ip_count'] = len(ips)\n",
        "        result['resolves'] = 1 if ips else 0\n",
        "    except Exception:\n",
        "        result['ip_count'] = 0\n",
        "        result['resolves'] = 0\n",
        "    finally:\n",
        "        socket.setdefaulttimeout(orig_timeout)\n",
        "    return result\n",
        "\n",
        "# ---------------------------\n",
        "# Core feature extraction per URL\n",
        "# ---------------------------\n",
        "def extract_lexical_features(url: str, perform_network_checks: bool = False) -> Dict[str, object]:\n",
        "    \"\"\"\n",
        "    Extract a dict of features for a single URL.\n",
        "    perform_network_checks: if True, includes lightweight DNS resolution features (may slow down).\n",
        "    \"\"\"\n",
        "    record = {}\n",
        "    if not url or not isinstance(url, str):\n",
        "        # return zeroed features\n",
        "        keys = [\n",
        "            'url_length','host_length','path_length','num_dots','num_path_segments','num_query_params',\n",
        "            'count_digits','count_letters','digit_letter_ratio','count_special_chars','num_hyphens','has_at',\n",
        "            'has_ip_in_host','tld','suspicious_tld','entropy_host','entropy_path','percent_encoded_frac',\n",
        "            'vowel_frac','has_https','has_port','has_fragment'\n",
        "        ]\n",
        "        return {k: 0 for k in keys}\n",
        "\n",
        "    norm = normalize_url(url)\n",
        "    parsed = urlparse(norm)\n",
        "    scheme = parsed.scheme.lower()\n",
        "    netloc = parsed.netloc\n",
        "    path = parsed.path or \"\"\n",
        "    query = parsed.query or \"\"\n",
        "    fragment = parsed.fragment or \"\"\n",
        "    hostname = netloc.split('@')[-1].split(':')[0]  # strip possible credentials and port\n",
        "\n",
        "    # basic lengths\n",
        "    record['url_length'] = len(norm)\n",
        "    record['host_length'] = len(hostname)\n",
        "    record['path_length'] = len(path)\n",
        "    record['num_dots'] = hostname.count('.')\n",
        "    # path segments\n",
        "    record['num_path_segments'] = len([p for p in path.split('/') if p])\n",
        "    # query params count (rough)\n",
        "    record['num_query_params'] = len([p for p in query.split('&') if '=' in p]) if query else 0\n",
        "\n",
        "    # counts\n",
        "    record['count_digits'] = count_digits(norm)\n",
        "    record['count_letters'] = count_letters(norm)\n",
        "    record['digit_letter_ratio'] = (record['count_digits'] / (record['count_letters'] + 1))\n",
        "    record['count_special_chars'] = count_special_chars(norm)\n",
        "    record['num_hyphens'] = norm.count('-')\n",
        "    record['has_at'] = 1 if '@' in norm else 0\n",
        "\n",
        "    # host checks\n",
        "    record['has_ip_in_host'] = is_ip_host(hostname)\n",
        "    registered_domain, tld = extract_tld_info(hostname)\n",
        "    record['registered_domain'] = registered_domain\n",
        "    record['tld'] = tld\n",
        "    record['suspicious_tld'] = 1 if (tld in SUSPICIOUS_TLDS) else 0\n",
        "\n",
        "    # entropy & encoding\n",
        "    record['entropy_host'] = shannon_entropy(hostname)\n",
        "    record['entropy_path'] = shannon_entropy(path)\n",
        "    record['percent_encoded_frac'] = percent_encoded_fraction(norm)\n",
        "\n",
        "    # misc heuristics\n",
        "    record['vowel_frac'] = vowel_fraction(hostname)\n",
        "    record['has_https'] = 1 if scheme == 'https' else 0\n",
        "    record['has_port'] = has_port(netloc)\n",
        "    record['has_fragment'] = 1 if fragment else 0\n",
        "\n",
        "    # heuristic suspicious tokens in URL/domain\n",
        "    suspicious_tokens = ['login', 'signin', 'secure', 'webscr', 'bank', 'verify', 'update', 'account', 'confirm']\n",
        "    url_lower = norm.lower()\n",
        "    for tok in suspicious_tokens:\n",
        "        record[f'token_{tok}'] = 1 if tok in url_lower else 0\n",
        "\n",
        "    # length buckets (useful as categorical-ish numeric)\n",
        "    record['url_len_bucket_<50'] = 1 if record['url_length'] < 50 else 0\n",
        "    record['url_len_bucket_50_100'] = 1 if 50 <= record['url_length'] < 100 else 0\n",
        "    record['url_len_bucket_100_200'] = 1 if 100 <= record['url_length'] < 200 else 0\n",
        "    record['url_len_bucket_>=200'] = 1 if record['url_length'] >= 200 else 0\n",
        "\n",
        "    # network checks (optional & cached by user)\n",
        "    if perform_network_checks:\n",
        "        netinfo = try_resolve_domain(hostname)\n",
        "        record['dns_ip_count'] = netinfo.get('ip_count', 0)\n",
        "        record['dns_resolves'] = netinfo.get('resolves', 0)\n",
        "    else:\n",
        "        record['dns_ip_count'] = None\n",
        "        record['dns_resolves'] = None\n",
        "\n",
        "    return record\n",
        "\n",
        "# ---------------------------\n",
        "# Batch processing helpers\n",
        "# ---------------------------\n",
        "def build_features_for_df(url_series: pd.Series,\n",
        "                          perform_network_checks: bool = False,\n",
        "                          char_maxlen: int = DEFAULT_MAX_LEN,\n",
        "                          save_prefix: Optional[str] = None\n",
        "                         ) -> Tuple[pd.DataFrame, np.ndarray, Dict]:\n",
        "    \"\"\"\n",
        "    Given a pandas Series of URLs, returns:\n",
        "      - features_df: DataFrame with extracted features (one row per URL)\n",
        "      - char_seqs: numpy array shape (n_urls, char_maxlen) with integer indices for Char-CNN\n",
        "      - meta: dict containing char_vocab mapping and any notes\n",
        "\n",
        "    If save_prefix provided (string path prefix), will save:\n",
        "      - {save_prefix}_features.csv\n",
        "      - {save_prefix}_char_seqs.npy\n",
        "      - {save_prefix}_meta.json\n",
        "    \"\"\"\n",
        "    urls = url_series.fillna(\"\").astype(str).tolist()\n",
        "    n = len(urls)\n",
        "\n",
        "    # Extract lexical features\n",
        "    feats = []\n",
        "    for i, u in enumerate(urls):\n",
        "        feats.append(extract_lexical_features(u, perform_network_checks=perform_network_checks))\n",
        "        if (i + 1) % 5000 == 0:\n",
        "            print(f\"Processed {i+1}/{n} URLs...\")\n",
        "\n",
        "    features_df = pd.DataFrame(feats)\n",
        "\n",
        "    # Drop columns that are non-numeric or not needed for ML; keep registered_domain and tld optionally\n",
        "    # We'll keep registered_domain for reference but ML models should not get it raw (hash or drop later)\n",
        "    # Convert None to np.nan as LightGBM handles nan\n",
        "    features_df = features_df.replace({None: np.nan})\n",
        "\n",
        "    # Build char sequences\n",
        "    char_seqs = np.zeros((n, char_maxlen), dtype=np.int32)\n",
        "    for i, u in enumerate(urls):\n",
        "        seq = encode_url_to_char_indices(u, maxlen=char_maxlen)\n",
        "        char_seqs[i, :] = seq\n",
        "\n",
        "    meta = {\n",
        "        \"char_to_index\": CHAR_TO_INDEX,\n",
        "        \"unk_char_index\": UNK_CHAR_INDEX,\n",
        "        \"maxlen\": char_maxlen\n",
        "    }\n",
        "\n",
        "    # Optional saving\n",
        "    if save_prefix:\n",
        "        os.makedirs(os.path.dirname(save_prefix), exist_ok=True)\n",
        "        features_csv = f\"{save_prefix}_features.csv\"\n",
        "        char_npy = f\"{save_prefix}_char_seqs.npy\"\n",
        "        meta_json = f\"{save_prefix}_meta.json\"\n",
        "        features_df.to_csv(features_csv, index=False)\n",
        "        np.save(char_npy, char_seqs)\n",
        "        with open(meta_json, \"w\") as f:\n",
        "            json.dump(meta, f)\n",
        "        print(f\"Saved features to {features_csv}, char seqs to {char_npy}, meta to {meta_json}\")\n",
        "\n",
        "    return features_df, char_seqs, meta\n",
        "\n",
        "# ---------------------------\n",
        "# Character encoding for Char-CNN\n",
        "# ---------------------------\n",
        "def encode_url_to_char_indices(url: str, maxlen: int = DEFAULT_MAX_LEN) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Turn URL into fixed-length vector of integer token ids.\n",
        "    - Lowercases before mapping\n",
        "    - Unknown chars map to UNK_CHAR_INDEX\n",
        "    - Right-pad with zeros (index 0 reserved for padding)\n",
        "    \"\"\"\n",
        "    if not isinstance(url, str):\n",
        "        url = \"\"\n",
        "    # normalize a bit but keep raw path/params signs\n",
        "    url = normalize_url(url)\n",
        "    url = url.lower()\n",
        "    seq = np.zeros(maxlen, dtype=np.int32)\n",
        "    # truncate from left → keep rightmost part (commonly path + file)\n",
        "    u = url[-maxlen:]\n",
        "    for i, ch in enumerate(u):\n",
        "        idx = CHAR_TO_INDEX.get(ch)\n",
        "        if idx is None:\n",
        "            idx = UNK_CHAR_INDEX\n",
        "        seq[i] = idx\n",
        "    return seq\n",
        "\n",
        "# ---------------------------\n",
        "# Small helper for hashing domains (for logging without storing raw URL)\n",
        "# ---------------------------\n",
        "def salted_hash(value: str, salt: str = \"static_salt_for_demo\") -> str:\n",
        "    if not isinstance(value, str):\n",
        "        value = \"\"\n",
        "    return hashlib.sha256((salt + value).encode()).hexdigest()\n",
        "\n",
        "# ---------------------------\n",
        "# Quick test utility\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Simple local smoke test (not heavy)\n",
        "    sample = [\n",
        "        \"http://192.168.0.1/login?user=abc\",\n",
        "        \"https://www.paypal.com/signin\",\n",
        "        \"http://secure-google-accounts.xyz/verify\",\n",
        "        \"https://accounts.google.com/ServiceLogin\",\n",
        "        \"phishing-domain.tk/login.php?acc=1\"\n",
        "    ]\n",
        "    df = pd.DataFrame({'url': sample})\n",
        "    feats, chars, meta = build_features_for_df(df['url'], perform_network_checks=False, save_prefix=None)\n",
        "    print(\"Features head:\")\n",
        "    print(feats.head())\n",
        "    print(\"Char seqs shape:\", chars.shape)\n",
        "PY\n",
        "echo \"Wrote feature_extraction.py\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BghIbxOEyX2i",
        "outputId": "e32e07e2-ad40-439d-ddaf-9536477559e5"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote feature_extraction.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import py_compile, traceback\n",
        "path = \"/content/drive/MyDrive/AI_Phishing_Detector/src/feature_extraction.py\"\n",
        "try:\n",
        "    py_compile.compile(path, doraise=True)\n",
        "    print(\"✅ Compile OK — no syntax errors.\")\n",
        "except Exception:\n",
        "    import traceback\n",
        "    print(\"❌ Compile error; traceback below:\")\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WYfFQyTyiF_",
        "outputId": "06cbbe87-06c2-43b1-8e68-e2ab23699d3a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Compile OK — no syntax errors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, importlib\n",
        "sys.path.append(\"/content/drive/MyDrive/AI_Phishing_Detector\")\n",
        "import src.feature_extraction as fe\n",
        "importlib.reload(fe)\n",
        "print(\"Module path:\", fe.__file__)\n",
        "print(\"Has build_features_for_df:\", hasattr(fe, \"build_features_for_df\"))\n",
        "# show first 10 names\n",
        "print([n for n in dir(fe) if n[0].islower()][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrtEzOX-zm5z",
        "outputId": "b42f1949-59d9-492d-a472-40d3aabd62a8"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Module path: /content/drive/MyDrive/AI_Phishing_Detector/src/feature_extraction.py\n",
            "Has build_features_for_df: True\n",
            "['build_features_for_df', 'count_digits', 'count_letters', 'count_special_chars', 'encode_url_to_char_indices', 'extract_lexical_features', 'extract_tld_info', 'has_port', 'hashlib', 'is_ip_host', 'json', 'math', 'normalize_url', 'np', 'os', 'pd', 'percent_encoded_fraction', 're', 'salted_hash', 'shannon_entropy', 'socket', 'tldextract', 'try_resolve_domain', 'unquote', 'urlparse', 'vowel_fraction']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from src.feature_extraction import build_features_for_df\n",
        "import pandas as pd\n",
        "\n",
        "sample = [\n",
        "    \"http://192.168.0.1/login?user=abc\",\n",
        "    \"https://www.paypal.com/signin\",\n",
        "    \"http://secure-google-accounts.xyz/verify\",\n",
        "    \"https://accounts.google.com/ServiceLogin\",\n",
        "    \"phishing-domain.tk/login.php?acc=1\"\n",
        "]\n",
        "feats, chars, meta = build_features_for_df(pd.Series(sample), perform_network_checks=False, char_maxlen=200, save_prefix=None)\n",
        "print(\"feats.shape:\", feats.shape)\n",
        "print(\"chars.shape:\", chars.shape)\n",
        "print(feats.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDO61Oi8zqo2",
        "outputId": "2e2ecf7c-5302-45b4-a19b-5bebc8cb2ccf"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "feats.shape: (5, 38)\n",
            "chars.shape: (5, 200)\n",
            "   url_length  host_length  path_length  num_dots  num_path_segments  \\\n",
            "0          33           11            6         3                  1   \n",
            "1          29           14            7         2                  1   \n",
            "2          40           26            7         1                  1   \n",
            "\n",
            "   num_query_params  count_digits  count_letters  digit_letter_ratio  \\\n",
            "0                 1             8             16            0.470588   \n",
            "1                 0             0             23            0.000000   \n",
            "2                 0             0             33            0.000000   \n",
            "\n",
            "   count_special_chars  ...  token_verify  token_update  token_account  \\\n",
            "0                    9  ...             0             0              0   \n",
            "1                    6  ...             0             0              0   \n",
            "2                    7  ...             1             0              1   \n",
            "\n",
            "  token_confirm url_len_bucket_<50  url_len_bucket_50_100  \\\n",
            "0             0                  1                      0   \n",
            "1             0                  1                      0   \n",
            "2             0                  1                      0   \n",
            "\n",
            "   url_len_bucket_100_200  url_len_bucket_>=200  dns_ip_count  dns_resolves  \n",
            "0                       0                     0           NaN           NaN  \n",
            "1                       0                     0           NaN           NaN  \n",
            "2                       0                     0           NaN           NaN  \n",
            "\n",
            "[3 rows x 38 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/AI_Phishing_Detector/src/feature_extraction.py:268: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  features_df = features_df.replace({None: np.nan})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from src.feature_extraction import build_features_for_df\n",
        "\n",
        "# full dataset features\n",
        "features_df, char_seqs, meta = build_features_for_df(\n",
        "    df['url'],\n",
        "    perform_network_checks=False,   # keep False for speed today\n",
        "    char_maxlen=200,\n",
        "    save_prefix=\"/content/drive/MyDrive/AI_Phishing_Detector/data/features/live_dataset\"\n",
        ")\n",
        "\n",
        "print(\"✅ Features extraction complete\")\n",
        "print(\"Features shape:\", features_df.shape)\n",
        "print(\"Char sequences shape:\", char_seqs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKKuvzmpzxjy",
        "outputId": "f10643b3-8ceb-4548-8821-f11f6c3a9a87"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 5000/99361 URLs...\n",
            "Processed 10000/99361 URLs...\n",
            "Processed 15000/99361 URLs...\n",
            "Processed 20000/99361 URLs...\n",
            "Processed 25000/99361 URLs...\n",
            "Processed 30000/99361 URLs...\n",
            "Processed 35000/99361 URLs...\n",
            "Processed 40000/99361 URLs...\n",
            "Processed 45000/99361 URLs...\n",
            "Processed 50000/99361 URLs...\n",
            "Processed 55000/99361 URLs...\n",
            "Processed 60000/99361 URLs...\n",
            "Processed 65000/99361 URLs...\n",
            "Processed 70000/99361 URLs...\n",
            "Processed 75000/99361 URLs...\n",
            "Processed 80000/99361 URLs...\n",
            "Processed 85000/99361 URLs...\n",
            "Processed 90000/99361 URLs...\n",
            "Processed 95000/99361 URLs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/AI_Phishing_Detector/src/feature_extraction.py:268: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  features_df = features_df.replace({None: np.nan})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features to /content/drive/MyDrive/AI_Phishing_Detector/data/features/live_dataset_features.csv, char seqs to /content/drive/MyDrive/AI_Phishing_Detector/data/features/live_dataset_char_seqs.npy, meta to /content/drive/MyDrive/AI_Phishing_Detector/data/features/live_dataset_meta.json\n",
            "✅ Features extraction complete\n",
            "Features shape: (99361, 38)\n",
            "Char sequences shape: (99361, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "FEATURES_CSV = \"/content/drive/MyDrive/AI_Phishing_Detector/data/features/live_dataset_features.csv\"\n",
        "\n",
        "df = pd.read_csv(FEATURES_CSV)\n",
        "\n",
        "# Quick overview\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"\\nColumns:\\n\", df.columns.tolist())\n",
        "print(\"\\nColumn types:\\n\", df.dtypes)\n",
        "print(\"\\nFirst 5 rows:\\n\", df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWoOAOgV1QS6",
        "outputId": "b190a3b3-f633-4e97-894e-44bb881fa41e"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (99361, 38)\n",
            "\n",
            "Columns:\n",
            " ['url_length', 'host_length', 'path_length', 'num_dots', 'num_path_segments', 'num_query_params', 'count_digits', 'count_letters', 'digit_letter_ratio', 'count_special_chars', 'num_hyphens', 'has_at', 'has_ip_in_host', 'registered_domain', 'tld', 'suspicious_tld', 'entropy_host', 'entropy_path', 'percent_encoded_frac', 'vowel_frac', 'has_https', 'has_port', 'has_fragment', 'token_login', 'token_signin', 'token_secure', 'token_webscr', 'token_bank', 'token_verify', 'token_update', 'token_account', 'token_confirm', 'url_len_bucket_<50', 'url_len_bucket_50_100', 'url_len_bucket_100_200', 'url_len_bucket_>=200', 'dns_ip_count', 'dns_resolves']\n",
            "\n",
            "Column types:\n",
            " url_length                  int64\n",
            "host_length                 int64\n",
            "path_length                 int64\n",
            "num_dots                    int64\n",
            "num_path_segments           int64\n",
            "num_query_params            int64\n",
            "count_digits                int64\n",
            "count_letters               int64\n",
            "digit_letter_ratio        float64\n",
            "count_special_chars         int64\n",
            "num_hyphens                 int64\n",
            "has_at                      int64\n",
            "has_ip_in_host              int64\n",
            "registered_domain          object\n",
            "tld                        object\n",
            "suspicious_tld              int64\n",
            "entropy_host              float64\n",
            "entropy_path              float64\n",
            "percent_encoded_frac      float64\n",
            "vowel_frac                float64\n",
            "has_https                   int64\n",
            "has_port                    int64\n",
            "has_fragment                int64\n",
            "token_login                 int64\n",
            "token_signin                int64\n",
            "token_secure                int64\n",
            "token_webscr                int64\n",
            "token_bank                  int64\n",
            "token_verify                int64\n",
            "token_update                int64\n",
            "token_account               int64\n",
            "token_confirm               int64\n",
            "url_len_bucket_<50          int64\n",
            "url_len_bucket_50_100       int64\n",
            "url_len_bucket_100_200      int64\n",
            "url_len_bucket_>=200        int64\n",
            "dns_ip_count              float64\n",
            "dns_resolves              float64\n",
            "dtype: object\n",
            "\n",
            "First 5 rows:\n",
            "    url_length  host_length  path_length  num_dots  num_path_segments  \\\n",
            "0          85           25           14         2                  1   \n",
            "1          30           22            1         2                  0   \n",
            "2          26           18            0         1                  0   \n",
            "3          27           19            0         1                  0   \n",
            "4          17            9            0         1                  0   \n",
            "\n",
            "   num_query_params  count_digits  count_letters  digit_letter_ratio  \\\n",
            "0                 1            19             57            0.327586   \n",
            "1                 0             1             23            0.041667   \n",
            "2                 0             0             22            0.000000   \n",
            "3                 0             0             23            0.000000   \n",
            "4                 0             0             13            0.000000   \n",
            "\n",
            "   count_special_chars  ...  token_verify  token_update  token_account  \\\n",
            "0                    9  ...             0             0              0   \n",
            "1                    6  ...             0             0              0   \n",
            "2                    4  ...             0             0              0   \n",
            "3                    4  ...             0             0              0   \n",
            "4                    4  ...             0             0              0   \n",
            "\n",
            "  token_confirm url_len_bucket_<50  url_len_bucket_50_100  \\\n",
            "0             0                  0                      1   \n",
            "1             0                  1                      0   \n",
            "2             0                  1                      0   \n",
            "3             0                  1                      0   \n",
            "4             0                  1                      0   \n",
            "\n",
            "   url_len_bucket_100_200  url_len_bucket_>=200  dns_ip_count  dns_resolves  \n",
            "0                       0                     0           NaN           NaN  \n",
            "1                       0                     0           NaN           NaN  \n",
            "2                       0                     0           NaN           NaN  \n",
            "3                       0                     0           NaN           NaN  \n",
            "4                       0                     0           NaN           NaN  \n",
            "\n",
            "[5 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Paths\n",
        "FEATURES_CSV = \"/content/drive/MyDrive/AI_Phishing_Detector/data/features/live_dataset_features.csv\"\n",
        "LGB_CSV = \"/content/drive/MyDrive/AI_Phishing_Detector/data/features/live_features_for_lgb.csv\"\n",
        "\n",
        "# Load features\n",
        "df = pd.read_csv(FEATURES_CSV)\n",
        "\n",
        "# Drop non-numeric columns\n",
        "df.drop(columns=['registered_domain', 'tld'], inplace=True)\n",
        "\n",
        "# Fill NaNs with 0\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Save LGB-ready CSV\n",
        "df.to_csv(LGB_CSV, index=False)\n",
        "print(f\"LGB-ready features saved at: {LGB_CSV}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWmPETZO1fiI",
        "outputId": "8d1bcf8a-fbe6-4bc0-db71-e5d72a1b3fb4"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGB-ready features saved at: /content/drive/MyDrive/AI_Phishing_Detector/data/features/live_features_for_lgb.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade lightgbm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oNGa--b1s1G",
        "outputId": "13413122-f82e-44de-bdb2-ce1c19557023"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lightgbm) (1.16.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/AI_Phishing_Detector/src/train_models.py\n",
        "# /content/drive/MyDrive/AI_Phishing_Detector/src/train_models.py\n",
        "\"\"\"\n",
        "Robust training script (LightGBM + Char-CNN) with calibration and ensemble.\n",
        "This version will attempt to install ONNX/ONNXRuntime if not present and export the Char-CNN to ONNX.\n",
        "Saves:\n",
        " - models/lgb_calibrated.pkl\n",
        " - models/lgb_raw.pkl\n",
        " - models/charcnn.pt\n",
        " - models/charcnn.onnx (best-effort)\n",
        " - models/ensemble_metadata.json\n",
        " - models/test_predictions.csv\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import joblib\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# ML libs\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score,\n",
        "    precision_recall_fscore_support, confusion_matrix,\n",
        "    accuracy_score\n",
        ")\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.base import clone # Import clone\n",
        "\n",
        "# Torch for Char-CNN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Repro\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Paths\n",
        "BASE = Path(\"/content/drive/MyDrive/AI_Phishing_Detector\")\n",
        "FEATURES_DIRS = [\n",
        "    BASE / \"data\" / \"features\",\n",
        "    BASE / \"data\" / \"processed\"\n",
        "]\n",
        "MODELS_DIR = BASE / \"models\"\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Find files helper\n",
        "def find_file(names):\n",
        "    for d in FEATURES_DIRS:\n",
        "        for name in names:\n",
        "            p = d / name\n",
        "            if p.exists():\n",
        "                return p\n",
        "    return None\n",
        "\n",
        "LGB_READY = find_file([\"live_features_for_lgb.csv\", \"live_features_for_lgb.csv\"])\n",
        "FEATURES_FULL = find_file([\"live_dataset_features.csv\", \"live_features.csv\"])\n",
        "CHAR_NPY = find_file([\"live_dataset_char_seqs.npy\", \"live_char_seqs.npy\", \"live_dataset_char_seqs.npy\"])\n",
        "CHAR_META = find_file([\"live_dataset_meta.json\", \"live_char_meta.json\", \"live_dataset_meta.json\"])\n",
        "DATASET_CSV = BASE / \"data\" / \"processed\" / \"live_dataset.csv\"\n",
        "TRAIN_IDX = BASE / \"data\" / \"processed\" / \"train_idx.npy\"\n",
        "TEST_IDX = BASE / \"data\" / \"processed\" / \"test_idx.npy\"\n",
        "\n",
        "if LGB_READY is None and FEATURES_FULL is None:\n",
        "    raise FileNotFoundError(\"No features CSV found. Ensure earlier feature extraction saved live_features_for_lgb.csv or live_dataset_features.csv in data/features or data/processed.\")\n",
        "if CHAR_NPY is None:\n",
        "    raise FileNotFoundError(\"Char seqs .npy not found. Ensure live_dataset_char_seqs.npy or live_char_seqs.npy exists under data/features.\")\n",
        "\n",
        "print(\"Using feature file (LGB-ready):\", LGB_READY if LGB_READY else \"(will derive from full features)\")\n",
        "print(\"Using full features file:\", FEATURES_FULL)\n",
        "print(\"Char seqs file:\", CHAR_NPY)\n",
        "print(\"Char meta file (optional):\", CHAR_META)\n",
        "print(\"Labels file:\", DATASET_CSV if DATASET_CSV.exists() else \"(not found; labels must be in features CSV)\")\n",
        "\n",
        "# Hyperparams\n",
        "CHAR_MAXLEN = 200\n",
        "CHAR_BATCH_SIZE = 256 if torch.cuda.is_available() else 64\n",
        "CHAR_EPOCHS = int(os.environ.get(\"CHAR_EPOCHS\", \"12\")) if torch.cuda.is_available() else int(os.environ.get(\"CHAR_EPOCHS\", \"6\"))\n",
        "CHAR_LR = 1e-3\n",
        "CHAR_WEIGHT = float(os.environ.get(\"CHAR_WEIGHT\", 0.6))\n",
        "LGB_WEIGHT = float(os.environ.get(\"LGB_WEIGHT\", 0.4))\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {DEVICE}; CHAR_BATCH_SIZE={CHAR_BATCH_SIZE}; CHAR_EPOCHS={CHAR_EPOCHS}\")\n",
        "\n",
        "# Utility\n",
        "def safe_read_csv(path):\n",
        "    p = Path(path)\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(p)\n",
        "    return pd.read_csv(p)\n",
        "\n",
        "# Load data\n",
        "print(\"Loading datasets...\")\n",
        "if LGB_READY and Path(LGB_READY).exists():\n",
        "    df_lgb = safe_read_csv(LGB_READY)\n",
        "else:\n",
        "    df_full = safe_read_csv(FEATURES_FULL)\n",
        "    for col in [\"registered_domain\", \"tld\"]:\n",
        "        if col in df_full.columns:\n",
        "            df_full = df_full.drop(columns=[col])\n",
        "    # Address FutureWarning: Downcasting behavior in `replace`\n",
        "    df_full = df_full.replace({None: np.nan}).fillna(0).infer_objects(copy=False)\n",
        "    df_lgb = df_full\n",
        "\n",
        "if \"label\" not in df_lgb.columns:\n",
        "    if DATASET_CSV.exists():\n",
        "        labels = safe_read_csv(DATASET_CSV)[\"label\"].values\n",
        "        if len(labels) != len(df_lgb):\n",
        "            raise ValueError(\"Label length mismatch\")\n",
        "        df_lgb[\"label\"] = labels\n",
        "    else:\n",
        "        raise ValueError(\"No 'label' column and no live_dataset.csv found\")\n",
        "\n",
        "char_seqs = np.load(CHAR_NPY)\n",
        "N = len(char_seqs)\n",
        "if len(df_lgb) != N:\n",
        "    raise ValueError(f\"Row count mismatch: tabular={len(df_lgb)} vs char_seqs={N}\")\n",
        "\n",
        "y = df_lgb[\"label\"].values\n",
        "X_tab = df_lgb.drop(columns=[\"label\"])\n",
        "\n",
        "# Splits (use saved indices if available)\n",
        "if TRAIN_IDX.exists() and TEST_IDX.exists():\n",
        "    train_idx = np.load(TRAIN_IDX)\n",
        "    test_idx = np.load(TEST_IDX)\n",
        "    tr_idx, val_idx = train_test_split(train_idx, test_size=0.125, random_state=SEED, stratify=y[train_idx])\n",
        "else:\n",
        "    tr_idx, test_idx = train_test_split(np.arange(N), test_size=0.2, random_state=SEED, stratify=y)\n",
        "    tr_idx, val_idx = train_test_split(tr_idx, test_size=0.125, random_state=SEED, stratify=y[tr_idx])\n",
        "\n",
        "print(f\"Sizes -> train: {len(tr_idx)}, val: {len(val_idx)}, test: {len(test_idx)}\")\n",
        "print(\"Class counts (train):\", np.bincount(y[tr_idx].astype(int)))\n",
        "\n",
        "# LightGBM training (sklearn API)\n",
        "def train_lightgbm_sklearn(X_tab, y, tr_idx, val_idx):\n",
        "    print(\"Training LightGBM (sklearn API)...\")\n",
        "    X_train = X_tab.iloc[tr_idx]\n",
        "    y_train = y[tr_idx]\n",
        "    X_val = X_tab.iloc[val_idx]\n",
        "    y_val = y[val_idx]\n",
        "\n",
        "    clf = lgb.LGBMClassifier(\n",
        "        n_estimators=1000,\n",
        "        learning_rate=0.05,\n",
        "        num_leaves=64,\n",
        "        min_child_samples=20,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=SEED,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        clf.fit(\n",
        "            X_train, y_train,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            eval_metric='auc',\n",
        "            early_stopping_rounds=50,\n",
        "            verbose=100\n",
        "        )\n",
        "    except TypeError:\n",
        "        print(\"Warning: early_stopping_rounds not supported in this LightGBM version; running fit without early stopping.\")\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "    joblib.dump(clf, MODELS_DIR / \"lgb_raw.pkl\")\n",
        "    print(\"Saved raw LGB sklearn model:\", MODELS_DIR / \"lgb_raw.pkl\")\n",
        "\n",
        "    print(\"Calibrating LightGBM probabilities (sigmoid) using validation set...\")\n",
        "    try:\n",
        "        # Use clone to avoid the UserWarning with cv='prefit'\n",
        "        calibrator = CalibratedClassifierCV(clone(clf), cv='prefit', method='sigmoid')\n",
        "        calibrator.fit(X_val, y_val)\n",
        "    except Exception as e:\n",
        "        print(\"Calibrator prefit failed:\", str(e))\n",
        "        # Fallback using cross-validation if prefit fails\n",
        "        calibrator = CalibratedClassifierCV(base_estimator=clone(clf), method='sigmoid', cv=3)\n",
        "        calibrator.fit(np.vstack([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
        "\n",
        "    joblib.dump(calibrator, MODELS_DIR / \"lgb_calibrated.pkl\")\n",
        "    print(\"Saved calibrated LGB model:\", MODELS_DIR / \"lgb_calibrated.pkl\")\n",
        "    return calibrator\n",
        "\n",
        "# Char-CNN model + training\n",
        "class CharDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, seqs, labels, indices):\n",
        "        self.seqs = seqs[indices]\n",
        "        self.labels = labels[indices].astype(np.float32)\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.seqs[idx]).long(), torch.tensor(self.labels[idx]).float()\n",
        "\n",
        "class CharCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=16, num_filters=128, fc_dim=64, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim, padding_idx=0)\n",
        "        self.conv3 = nn.Conv1d(in_channels=embed_dim, out_channels=num_filters, kernel_size=3, padding=1)\n",
        "        self.conv5 = nn.Conv1d(in_channels=embed_dim, out_channels=num_filters, kernel_size=5, padding=2)\n",
        "        self.conv7 = nn.Conv1d(in_channels=embed_dim, out_channels=num_filters, kernel_size=7, padding=3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(num_filters * 3, fc_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(fc_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        emb = emb.permute(0, 2, 1)\n",
        "        o3 = self.pool(self.relu(self.conv3(emb))).squeeze(-1)\n",
        "        o5 = self.pool(self.relu(self.conv5(emb))).squeeze(-1)\n",
        "        o7 = self.pool(self.relu(self.conv7(emb))).squeeze(-1)\n",
        "        cat = torch.cat([o3, o5, o7], dim=1)\n",
        "        out = self.fc(cat).squeeze(-1)\n",
        "        return out\n",
        "\n",
        "def ensure_onnx_installed():\n",
        "    \"\"\"Try importing onnx; if missing, attempt pip install (best-effort).\"\"\"\n",
        "    try:\n",
        "        import onnx  # noqa: F401\n",
        "        return True\n",
        "    except Exception:\n",
        "        print(\"onnx not installed; attempting to install onnx and onnxruntime via pip...\")\n",
        "        try:\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"onnx\", \"onnxruntime\"], check=True, stdout=subprocess.PIPE)\n",
        "            import onnx  # noqa: F401\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(\"Failed to install onnx packages:\", e)\n",
        "            return False\n",
        "\n",
        "def export_charcnn_to_onnx(model, vocab_size, onnx_path):\n",
        "    \"\"\"Attempt ONNX export with new exporter when possible, fallback otherwise.\"\"\"\n",
        "    try:\n",
        "        installed = ensure_onnx_installed()\n",
        "        if not installed:\n",
        "            print(\"ONNX not available; skipping ONNX export.\")\n",
        "            return False\n",
        "        # prepare cpu copy\n",
        "        model_cpu = CharCNN(vocab_size=vocab_size)\n",
        "        model_cpu.load_state_dict({k.replace('module.',''):v for k,v in model.state_dict().items()})\n",
        "        model_cpu.eval()\n",
        "        dummy = torch.randint(0, vocab_size, (1, CHAR_MAXLEN), dtype=torch.long)\n",
        "        # Try new exporter (dynamo) first\n",
        "        try:\n",
        "            torch.onnx.export(\n",
        "                model_cpu,\n",
        "                dummy,\n",
        "                str(onnx_path),\n",
        "                input_names=[\"input\"],\n",
        "                output_names=[\"output\"],\n",
        "                opset_version=14,\n",
        "                dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
        "                verbose=False,\n",
        "                export_params=True,\n",
        "                do_constant_folding=True,\n",
        "                training=torch.onnx.TrainingMode.EVAL,\n",
        "                dynamo=True\n",
        "            )\n",
        "            print(\"Exported Char-CNN to ONNX (dynamo):\", onnx_path)\n",
        "            return True\n",
        "        except TypeError as e:\n",
        "            # older torch versions or unsupported arg; fallback to legacy export\n",
        "            print(\"dynamo export failed or not supported, falling back to legacy ONNX export:\", e)\n",
        "        except Exception as e:\n",
        "            print(\"dynamo export raised exception, will try legacy exporter:\", e)\n",
        "\n",
        "        try:\n",
        "            torch.onnx.export(\n",
        "                model_cpu,\n",
        "                dummy,\n",
        "                str(onnx_path),\n",
        "                input_names=[\"input\"],\n",
        "                output_names=[\"output\"],\n",
        "                opset_version=14,\n",
        "                dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
        "                verbose=False,\n",
        "                export_params=True,\n",
        "                do_constant_folding=True\n",
        "            )\n",
        "            print(\"Exported Char-CNN to ONNX (legacy):\", onnx_path)\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(\"Legacy ONNX export failed:\", e)\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(\"ONNX export overall failed:\", e)\n",
        "        return False\n",
        "\n",
        "def train_charcnn(seqs, labels, tr_idx, val_idx, device):\n",
        "    # local imports to avoid any odd scoping/resolution issues in some runtimes\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    print(\"Training Char-CNN...\")\n",
        "    vocab_size = int(seqs.max()) + 2\n",
        "    model = CharCNN(vocab_size=vocab_size).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=CHAR_LR)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    train_ds = CharDataset(seqs, labels, tr_idx)\n",
        "    val_ds = CharDataset(seqs, labels, val_idx)\n",
        "    num_workers = 2 if os.name != 'nt' else 0\n",
        "    train_loader = DataLoader(train_ds, batch_size=CHAR_BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=CHAR_BATCH_SIZE*2, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "    best_auc = 0.0\n",
        "    best_state = None\n",
        "    patience = 3\n",
        "    wait = 0\n",
        "    for epoch in range(1, CHAR_EPOCHS + 1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        cnt = 0\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * xb.size(0)\n",
        "            cnt += xb.size(0)\n",
        "        epoch_loss = total_loss / max(1, cnt)\n",
        "\n",
        "        model.eval()\n",
        "        preds = []\n",
        "        trues = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb = xb.to(device)\n",
        "                yb = yb.to(device)\n",
        "                logits = model(xb)\n",
        "                probs = torch.sigmoid(logits).cpu().numpy()\n",
        "                preds.append(probs)\n",
        "                trues.append(yb.cpu().numpy())\n",
        "        preds = np.concatenate(preds).ravel()\n",
        "        trues = np.concatenate(trues).ravel()\n",
        "        try:\n",
        "            auc = roc_auc_score(trues, preds)\n",
        "        except Exception:\n",
        "            auc = 0.0\n",
        "        print(f\"Epoch {epoch}/{CHAR_EPOCHS} — loss: {epoch_loss:.4f}, val_auc: {auc:.4f}\")\n",
        "        if auc > best_auc + 1e-4:\n",
        "            best_auc = auc\n",
        "            best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(\"Early stopping Char-CNN (patience reached).\")\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    torch.save(model.state_dict(), MODELS_DIR / \"charcnn.pt\")\n",
        "    print(\"Saved Char-CNN state to\", MODELS_DIR / \"charcnn.pt\")\n",
        "\n",
        "    # ONNX export attempt (best-effort)\n",
        "    onnx_path = MODELS_DIR / \"charcnn.onnx\"\n",
        "    exported = export_charcnn_to_onnx(model, int(seqs.max()) + 2, onnx_path)\n",
        "    if not exported:\n",
        "        print(\"ONNX export skipped or failed. If you want ONNX, install 'onnx' and 'onnxruntime' then re-run export.\")\n",
        "    return model\n",
        "\n",
        "# Evaluation helpers\n",
        "def evaluate_predictions(y_true, y_prob, threshold=0.5):\n",
        "    y_pred = (y_prob >= threshold).astype(int)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    auc = roc_auc_score(y_true, y_prob)\n",
        "    pr_auc = average_precision_score(y_true, y_prob)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    return dict(accuracy=float(acc), roc_auc=float(auc), pr_auc=float(pr_auc), precision=float(p), recall=float(r), f1=float(f1), cm=cm.tolist())\n",
        "\n",
        "def ensemble_predict(lgb_calib, char_model, X_tab, char_seqs_array, indices, device):\n",
        "    batch_idx = np.array(indices)\n",
        "    X_subset = X_tab.iloc[batch_idx]\n",
        "    lgb_prob = lgb_calib.predict_proba(X_subset)[:,1]\n",
        "    char_probs = []\n",
        "    char_model.eval()\n",
        "    bs = 1024\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(batch_idx), bs):\n",
        "            idxs = batch_idx[i:i+bs]\n",
        "            xb = torch.from_numpy(char_seqs_array[idxs]).long().to(device)\n",
        "            logits = char_model(xb)\n",
        "            probs = torch.sigmoid(logits).cpu().numpy()\n",
        "            char_probs.append(probs)\n",
        "    char_prob = np.concatenate(char_probs).ravel()\n",
        "    final_prob = CHAR_WEIGHT * char_prob + LGB_WEIGHT * lgb_prob\n",
        "    return final_prob, char_prob, lgb_prob\n",
        "\n",
        "# Main\n",
        "def main():\n",
        "    t0 = time.time()\n",
        "    calibrator = train_lightgbm_sklearn(X_tab, y, tr_idx, val_idx)\n",
        "    char_model = train_charcnn(char_seqs, y, tr_idx, val_idx, DEVICE)\n",
        "\n",
        "    meta_out = {\n",
        "        \"date\": datetime.now().astimezone().isoformat(),\n",
        "        \"seed\": SEED,\n",
        "        \"char_maxlen\": CHAR_MAXLEN,\n",
        "        \"char_epochs\": CHAR_EPOCHS,\n",
        "        \"char_batch_size\": CHAR_BATCH_SIZE,\n",
        "        \"ensemble_weights\": {\"char\": CHAR_WEIGHT, \"lgb\": LGB_WEIGHT}\n",
        "    }\n",
        "    with open(MODELS_DIR / \"ensemble_metadata.json\", \"w\") as f:\n",
        "        json.dump(meta_out, f, indent=2)\n",
        "    print(\"Saved ensemble metadata.\")\n",
        "\n",
        "    print(\"Evaluating on test set...\")\n",
        "    final_prob, char_prob, lgb_prob = ensemble_predict(calibrator, char_model, X_tab, char_seqs, test_idx, DEVICE)\n",
        "    metrics = evaluate_predictions(y[test_idx], final_prob, threshold=0.5)\n",
        "    print(\"ENSEMBLE METRICS:\")\n",
        "    print(json.dumps(metrics, indent=2))\n",
        "\n",
        "    out_df = pd.DataFrame({\n",
        "        \"url_index\": test_idx,\n",
        "        \"y_true\": y[test_idx],\n",
        "        \"prob_ensemble\": final_prob,\n",
        "        \"prob_char\": char_prob,\n",
        "        \"prob_lgb\": lgb_prob\n",
        "    })\n",
        "    out_df.to_csv(MODELS_DIR / \"test_predictions.csv\", index=False)\n",
        "    print(\"Saved test predictions to\", MODELS_DIR / \"test_predictions.csv\")\n",
        "    print(\"Elapsed (minutes):\", (time.time() - t0)/60.0)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xudtW16Jz9nF",
        "outputId": "b4934112-08b6-470c-8677-c0fb80522821"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/AI_Phishing_Detector/src/train_models.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/AI_Phishing_Detector/src/train_models.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6vdSz4m3RoD",
        "outputId": "06e8ea2a-3d7b-4ee8-dc0c-5b3592d4493f"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using feature file (LGB-ready): /content/drive/MyDrive/AI_Phishing_Detector/data/features/live_features_for_lgb.csv\n",
            "Using full features file: /content/drive/MyDrive/AI_Phishing_Detector/data/features/live_dataset_features.csv\n",
            "Char seqs file: /content/drive/MyDrive/AI_Phishing_Detector/data/features/live_dataset_char_seqs.npy\n",
            "Char meta file (optional): /content/drive/MyDrive/AI_Phishing_Detector/data/features/live_dataset_meta.json\n",
            "Labels file: /content/drive/MyDrive/AI_Phishing_Detector/data/processed/live_dataset.csv\n",
            "Device: cuda; CHAR_BATCH_SIZE=256; CHAR_EPOCHS=12\n",
            "Loading datasets...\n",
            "Sizes -> train: 69552, val: 9936, test: 19873\n",
            "Class counts (train): [34998 34554]\n",
            "Training LightGBM (sklearn API)...\n",
            "Warning: early_stopping_rounds not supported in this LightGBM version; running fit without early stopping.\n",
            "[LightGBM] [Info] Number of positive: 34554, number of negative: 34998\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017954 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2191\n",
            "[LightGBM] [Info] Number of data points in the train set: 69552, number of used features: 33\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496808 -> initscore=-0.012768\n",
            "[LightGBM] [Info] Start training from score -0.012768\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Saved raw LGB sklearn model: /content/drive/MyDrive/AI_Phishing_Detector/models/lgb_raw.pkl\n",
            "Calibrating LightGBM probabilities (sigmoid) using validation set...\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
            "  warnings.warn(\n",
            "Saved calibrated LGB model: /content/drive/MyDrive/AI_Phishing_Detector/models/lgb_calibrated.pkl\n",
            "Training Char-CNN...\n",
            "Epoch 1/12 — loss: 0.0656, val_auc: 0.9996\n",
            "Epoch 2/12 — loss: 0.0145, val_auc: 0.9996\n",
            "Epoch 3/12 — loss: 0.0105, val_auc: 0.9996\n",
            "Epoch 4/12 — loss: 0.0082, val_auc: 0.9995\n",
            "Early stopping Char-CNN (patience reached).\n",
            "Saved Char-CNN state to /content/drive/MyDrive/AI_Phishing_Detector/models/charcnn.pt\n",
            "onnx not installed; attempting to install onnx and onnxruntime via pip...\n",
            "dynamo export raised exception, will try legacy exporter: No module named 'onnxscript'\n",
            "/content/drive/MyDrive/AI_Phishing_Detector/src/train_models.py:276: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(\n",
            "Exported Char-CNN to ONNX (legacy): /content/drive/MyDrive/AI_Phishing_Detector/models/charcnn.onnx\n",
            "Saved ensemble metadata.\n",
            "Evaluating on test set...\n",
            "ENSEMBLE METRICS:\n",
            "{\n",
            "  \"accuracy\": 0.9979872188396317,\n",
            "  \"roc_auc\": 0.9994658867618756,\n",
            "  \"pr_auc\": 0.99962433464975,\n",
            "  \"precision\": 0.9995935372421502,\n",
            "  \"recall\": 0.9963536918869644,\n",
            "  \"f1\": 0.9979709850867404,\n",
            "  \"cm\": [\n",
            "    [\n",
            "      9996,\n",
            "      4\n",
            "    ],\n",
            "    [\n",
            "      36,\n",
            "      9837\n",
            "    ]\n",
            "  ]\n",
            "}\n",
            "Saved test predictions to /content/drive/MyDrive/AI_Phishing_Detector/models/test_predictions.csv\n",
            "Elapsed (minutes): 0.6036757866541544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install runtime if missing\n",
        "!pip install onnxruntime --quiet"
      ],
      "metadata": {
        "id": "kC-hmQRh4hfr"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "state_dict = torch.load(\"/content/drive/MyDrive/AI_Phishing_Detector/models/charcnn.pt\")\n",
        "emb_weight = state_dict['embedding.weight']\n",
        "vocab_size = emb_weight.shape[0]\n",
        "print(\"ONNX expects vocab_size =\", vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TyQr4yQ7trA",
        "outputId": "01e8b7f5-69ea-4bd3-9155-d6754e6536cd"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX expects vocab_size = 52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy = np.random.randint(0, vocab_size, size=(1, 200)).astype(np.int64)"
      ],
      "metadata": {
        "id": "WImS1WWe7w18"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "sess = ort.InferenceSession(\"/content/drive/MyDrive/AI_Phishing_Detector/models/charcnn.onnx\")\n",
        "out = sess.run(None, {\"input\": dummy})\n",
        "print(\"ONNX output shape:\", np.array(out[0]).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2rx76U98M_8",
        "outputId": "8eb88719-a877-4e3f-d966-90b4cc7147e8"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX output shape: (1,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X_tab: your tabular features DataFrame\n",
        "# y: labels array\n",
        "\n",
        "N = len(y)\n",
        "# Stratified split: 20% test, then 12.5% of remaining for validation\n",
        "tr_idx, test_idx = train_test_split(np.arange(N), test_size=0.2, random_state=42, stratify=y)\n",
        "tr_idx, val_idx = train_test_split(tr_idx, test_size=0.125, random_state=42, stratify=y[tr_idx])\n",
        "\n",
        "print(\"Sizes -> train:\", len(tr_idx), \"val:\", len(val_idx), \"test:\", len(test_idx))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Bn-Ohnr86e6",
        "outputId": "cde9a814-0f34-4ad0-b216-6844b57c17bf"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sizes -> train: 69559 val: 9938 test: 19875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calibrated LightGBM\n",
        "import joblib\n",
        "lgb_calib = joblib.load(\"/content/drive/MyDrive/AI_Phishing_Detector/models/lgb_calibrated.pkl\")\n",
        "\n",
        "# Char-CNN char sequences numpy array\n",
        "char_seqs = np.load(\"/content/drive/MyDrive/AI_Phishing_Detector/data/features/live_dataset_char_seqs.npy\")\n",
        "\n",
        "# Tabular features DataFrame\n",
        "import pandas as pd\n",
        "X_tab = pd.read_csv(\"/content/drive/MyDrive/AI_Phishing_Detector/data/features/live_features_for_lgb.csv\")\n",
        "y = pd.read_csv(\"/content/drive/MyDrive/AI_Phishing_Detector/data/processed/live_dataset.csv\")[\"label\"].values"
      ],
      "metadata": {
        "id": "VuEaNSFM8PGY"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "# Load ONNX session\n",
        "onnx_path = \"/content/drive/MyDrive/AI_Phishing_Detector/models/charcnn.onnx\"\n",
        "sess = ort.InferenceSession(onnx_path)\n",
        "input_name = sess.get_inputs()[0].name\n",
        "output_name = sess.get_outputs()[0].name\n",
        "\n",
        "# Ensemble prediction function\n",
        "def ensemble_predict_onnx(lgb_calib, char_seqs_array, X_tab, indices, char_weight=0.6, lgb_weight=0.4):\n",
        "    batch_idx = np.array(indices)\n",
        "    N_chars = char_seqs_array.shape[0]\n",
        "\n",
        "    # Clip indices to avoid out-of-bounds\n",
        "    safe_idx = batch_idx[batch_idx < N_chars]\n",
        "    if len(safe_idx) < len(batch_idx):\n",
        "        print(f\"Dropped {len(batch_idx)-len(safe_idx)} indices out-of-bounds for char_seqs\")\n",
        "\n",
        "    # LGB predictions\n",
        "    X_subset = X_tab.iloc[safe_idx]\n",
        "    lgb_prob = lgb_calib.predict_proba(X_subset)[:,1]\n",
        "\n",
        "    # Char-CNN ONNX predictions\n",
        "    char_probs = []\n",
        "    vocab_size = int(char_seqs_array.max()) + 1\n",
        "    bs = 1024\n",
        "    for i in range(0, len(safe_idx), bs):\n",
        "        idxs = safe_idx[i:i+bs]\n",
        "        xb = char_seqs_array[idxs]\n",
        "        xb = np.clip(xb, 0, vocab_size-1).astype(np.int64)\n",
        "        out = sess.run([output_name], {input_name: xb})\n",
        "        char_probs.append(out[0].ravel())\n",
        "\n",
        "    char_prob = np.concatenate(char_probs)\n",
        "\n",
        "    # Weighted ensemble\n",
        "    final_prob = char_weight * char_prob + lgb_weight * lgb_prob\n",
        "    return final_prob, char_prob, lgb_prob\n",
        "\n",
        "# Run on test set\n",
        "final_prob, char_prob, lgb_prob = ensemble_predict_onnx(\n",
        "    lgb_calib,\n",
        "    char_seqs,\n",
        "    X_tab,\n",
        "    test_idx,\n",
        "    char_weight=0.6,\n",
        "    lgb_weight=0.4\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwBHA1-18gtA",
        "outputId": "0b891de3-319b-4d63-ca68-21f0d16603d9"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped 2 indices out-of-bounds for char_seqs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ensemble_predict_onnx should return the indices actually used\n",
        "def ensemble_predict_onnx(lgb_calib, char_seqs_array, X_tab, indices, char_weight=0.6, lgb_weight=0.4):\n",
        "    batch_idx = np.array(indices)\n",
        "    N_chars = char_seqs_array.shape[0]\n",
        "\n",
        "    # Clip indices to avoid out-of-bounds\n",
        "    safe_idx = batch_idx[batch_idx < N_chars]\n",
        "    if len(safe_idx) < len(batch_idx):\n",
        "        print(f\"Dropped {len(batch_idx)-len(safe_idx)} indices out-of-bounds for char_seqs\")\n",
        "\n",
        "    # LGB predictions\n",
        "    X_subset = X_tab.iloc[safe_idx]\n",
        "    lgb_prob = lgb_calib.predict_proba(X_subset)[:,1]\n",
        "\n",
        "    # Char-CNN ONNX predictions\n",
        "    char_probs = []\n",
        "    vocab_size = int(char_seqs_array.max()) + 1\n",
        "    bs = 1024\n",
        "    for i in range(0, len(safe_idx), bs):\n",
        "        idxs = safe_idx[i:i+bs]\n",
        "        xb = char_seqs_array[idxs]\n",
        "        xb = np.clip(xb, 0, vocab_size-1).astype(np.int64)\n",
        "        out = sess.run([output_name], {input_name: xb})\n",
        "        char_probs.append(out[0].ravel())\n",
        "\n",
        "    char_prob = np.concatenate(char_probs)\n",
        "\n",
        "    # Weighted ensemble\n",
        "    final_prob = char_weight * char_prob + lgb_weight * lgb_prob\n",
        "    return final_prob, char_prob, lgb_prob, safe_idx  # return safe indices\n",
        "\n",
        "# Run ensemble ONNX\n",
        "final_prob, char_prob, lgb_prob, safe_idx = ensemble_predict_onnx(\n",
        "    lgb_calib, char_seqs, X_tab, test_idx\n",
        ")\n",
        "\n",
        "# Now align y_true to safe_idx\n",
        "y_true_safe = y[safe_idx]\n",
        "\n",
        "# Evaluate\n",
        "metrics = evaluate_predictions(y_true_safe, final_prob)\n",
        "print(\"ENSEMBLE METRICS:\")\n",
        "print(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo2s2XCz-A7_",
        "outputId": "2928af2f-f732-49ae-935e-33254bb15d32"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped 2 indices out-of-bounds for char_seqs\n",
            "ENSEMBLE METRICS:\n",
            "{'accuracy': 0.9977356211945856, 'roc_auc': 0.9993460693693665, 'pr_auc': 0.9995539080375587, 'precision': 0.9996921498204208, 'recall': 0.9957072771872445, 'f1': 0.99769573454862, 'cm': [[10086, 3], [42, 9742]]}\n"
          ]
        }
      ]
    }
  ]
}